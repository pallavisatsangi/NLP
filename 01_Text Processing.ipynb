{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b323ce0",
   "metadata": {},
   "source": [
    "# Text Preprocessing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666b458",
   "metadata": {},
   "source": [
    "### Imports Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00ceea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk  --- use pip install if nltk is not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd253409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae505d24",
   "metadata": {},
   "source": [
    "# Removing Noise:\n",
    "Noise removal involves getting rid of unwanted characters, symbols, and formatting from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c248bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise1(text):\n",
    "    # Remove special characters, digits, and replaces multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f02365e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellooo How r u Welcome\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Hellooo.    How r u???&^&^ Welcome##### 43434\"\n",
    "cleaned_text = remove_noise1(text)\n",
    "print(cleaned_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0e665",
   "metadata": {},
   "source": [
    "# Convert Text to Lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9eaea8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5648826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helllooooooo.    how r uuuuuu ???&^&^ welcome##### 43434\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"HELLLOOOOOOO.    How R UUUUUU ???&^&^ Welcome##### 43434\"\n",
    "cleaned_text = convert_to_lowercase(text)\n",
    "print(cleaned_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5352e5",
   "metadata": {},
   "source": [
    "# Removing Numerical Digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0dd26fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(text):\n",
    "    return re.sub(r'\\d+', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc14970a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLLOOOOOOO.    How R UUUUUU ???&^&^ Welcome##### \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"HELLLOOOOOOO.    How R UUUUUU ???&^&^ Welcome##### 43434\"\n",
    "cleaned_text = remove_digits(text)\n",
    "print(cleaned_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c62d1",
   "metadata": {},
   "source": [
    "# Removing URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52319e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8783941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLLOOOOOOO.    How R UUUUUU ???&^&^ Welcome##### 43434 \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"HELLLOOOOOOO.    How R UUUUUU ???&^&^ Welcome##### 43434 http://www.google.in\"\n",
    "cleaned_text = remove_urls(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a999ee0",
   "metadata": {},
   "source": [
    "# Removing HTML Tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "482b828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    from bs4 import BeautifulSoup\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f46eed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLLOOOOOOO.    How R UUUUUU ???&^&^ Welcome##### 43434   WOW \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"HELLLOOOOOOO.    How R UUUUUU ???&^&^ Welcome##### 43434  <B> WOW <B>\"\n",
    "cleaned_text = remove_html_tags(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4048c2",
   "metadata": {},
   "source": [
    "# Handling Stopwords:\n",
    "Stopwords are common words like \"the,\" \"and,\" \"is,\" etc., which often don't add much meaning to the text and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b367a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pallavisatsangi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4b64ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'breaks', 'text', 'individual', 'words', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "tokens = ['Tokenization', 'breaks', 'the', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.']\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(filtered_tokens)  # Output: ['Tokenization', 'breaks', 'text', 'individual', 'words', 'tokens', '.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de335a",
   "metadata": {},
   "source": [
    "# Tokenization:\n",
    "Tokenization breaks the text into individual words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5c4aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc62bd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'breaks', 'the', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Tokenization breaks the text into individual words or tokens.\"\n",
    "tokens = tokenize_text(text)\n",
    "print(tokens)  # Output: ['Tokenization', 'breaks', 'the', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad52077",
   "metadata": {},
   "source": [
    "# Stemming:\n",
    "Stemming reduces words to their root or base form by removing suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eeed18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stem_text(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8fabafd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', 'break', 'the', 'text', 'into', 'individu', 'word', 'or', 'token', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "tokens = ['Tokenization', 'breaks', 'the', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.']\n",
    "stemmed_tokens = stem_text(tokens)\n",
    "print(stemmed_tokens)  # Output: ['token', 'break', 'the', 'text', 'into', 'individu', 'word', 'or', 'token', '.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb5a8a",
   "metadata": {},
   "source": [
    "# Lemmatization:\n",
    "Lemmatization reduces words to their base or dictionary form (lemma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b937a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d933c1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'break', 'the', 'text', 'into', 'individual', 'word', 'or', 'token', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "tokens = ['Tokenization', 'breaks', 'the', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.']\n",
    "lemmatized_tokens = lemmatize_text(tokens)\n",
    "print(lemmatized_tokens)  # Output: ['Tokenization', 'break', 'the', 'text', 'into', 'individual', 'word', 'or', 'token', '.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce36db",
   "metadata": {},
   "source": [
    "# Compare,Combine, remove duplicate words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcf65a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strings are equal.\n",
      "Combined two strings: hello world hello world\n",
      "Combined strings without duplicates: world hello\n"
     ]
    }
   ],
   "source": [
    "def compare_strings(string1, string2):\n",
    "    return string1 == string2\n",
    "\n",
    "def combine_two_strings(string1, string2):\n",
    "    return string1 + \" \" + string2\n",
    "\n",
    "def combine_strings_remove_duplicates(string1, string2):\n",
    "    words1 = set(string1.split())\n",
    "    words2 = set(string2.split())\n",
    "    combined_words = words1.union(words2)\n",
    "    return \" \".join(combined_words)\n",
    "\n",
    "# Example usage\n",
    "string1 = \"hello world\"\n",
    "string2 = \"hello world\"\n",
    "if compare_strings(string1, string2):\n",
    "    print(\"Strings are equal.\")\n",
    "else:\n",
    "    print(\"Strings are not equal.\")\n",
    "\n",
    "combined_string = combine_two_strings(string1, string2)\n",
    "print(\"Combined two strings:\", combined_string)\n",
    "\n",
    "combined_without_duplicates = combine_strings_remove_duplicates(string1, string2)\n",
    "print(\"Combined strings without duplicates:\", combined_without_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a017b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
